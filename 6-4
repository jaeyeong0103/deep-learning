MLE (Maximum Likelihood Extimation)
BCE 와 MSE 는 loss를 최소화 하는 파라미터를 찾는 과정이 MLE, 관측된 Measurement를 찾는 가능성을 최대화 하는 파라미터를 찾는 과정과 같다
Likelihood 라는 공통된 루트를 가짐 likelihood란 어떤 사건이 실제로 발생했을 때, 그 사건이 특정한 원인이나 조건에 의해 발생했을 가능성을 나타내는 개념
MLE는 이 입력에는 이 출력이 나올 확률을 키워야 해 라는 관점을 가지고 있으면 입출력 간의 확률을 올린다 
신경망(웨이트)를 바꿔가면서 likelihood가 가장 작은 값을 찾는다
관측된 measurement의 발생 가능성을 최대화 하려는 통계적 추정 과정이라는 의미를 가진다
likelihood 는 배르누이드 분포 식을 가정하고 만든 식이다 그래서 likelihood는 특정 분포에 국한되지 않고 다양한 통계 모델에서 이용 가능하다
likelihood는 확률 분포식과 형태가 유사하며, 식 자체가 아니라 함수적으로 이해해야 한다.
데이터가 여러개일 경우 독립시행이라 분포의 곱이 되고 -log를 취하게 된다면 BCE loss가 된다
likelihood에 음수를 취한 것을 NLL이라 한다. MSE Loss는 가우시안 분포를 가정할 때 NLL을 구할 수 있다
결론적으로, BCE와 MSE는 각각 이진 분포와 정규 분포를 기반으로 한 손실 함수이며, 둘 다 NLL을 최소화하는 관점에서 해석될 수 있다.
MAE loss 는 라플라스 분포를 가정했을 때 나오는 loss의 함수로 정의 할 수 있다
BCE loss가 민감한 이유는 베르누이 분포에서 찾아 볼 수 있는데 베르누이 분포가 이진 분류 문제에 본질에 더 적합한 가정이기 때문이다
회귀문제에서는 MSE loss를 주로 사용하게 되는데 그 이유는 회귀에서는 레이블이 연속적인 값을 가질 수 있으므로 0 또는 1에 제한되는 베르누이분포보단
가우시안 분포가 더 적합하다. 데이터가 outlier가 많을 경우 라플라스 분포를 가정하는 것이 유리하다 라플라스 분포는 가우시안 분포에 비해 꼬리 부분에
영향을 많이 받기 때문에 outlier의 데이터를 영향을 적게 받기에 MSE loss가 더욱 적합하다
해결하려는 문제에 대해 가장 잘 맞는 분포를 선택함으로써 적절한 loss 함수를 결정하는 관점에서 ai의 학습은 w(파라미터) 를 추정하기 위한MLE를 수행하는 과정이라고 해석할 수 있다
그래서 MLE 딥러닝은 기존의 loss함수중애서 하이퍼파라미터를 선택하는 것을 넘어 문제에 특성에 맞게 새로운 확률분포를 가정하고 이에 기반한 loss 함수를직접 설계 할 수 있다
결국 인공신경망은 MLE 기계가 된다




