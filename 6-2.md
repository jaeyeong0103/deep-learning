sigmoid를 이용한 이진 분류
총 파라미터 개수 = 그래디언트 길이(백터의 크기)
모든 데이터를 사용할 때 GD
하나의 데이터만 사용할 때 SGD
일부 데이터를 사용할 때 Mini-Batch GD
이전 그래디언트 누적 방식 Momentum
Momentum + RMSProp = Adam
loss 함수 선택 MSE BCE(이진 분류에 적합함)
BCE loss 
 이 입력에는 이 출력이 나와야 해 를 loss함수로 표현하게 되면 loss를 최대한 줄여야해 가 됨
 입력이 들어오면 원하는 출력값이 나오도록 loss를 정의하면 됨(확률)
Mini-Batch
각 데이터의 추출이 서로 독립적이기 때문에 여러 데이터에 대한 예측 확률을 곱해야 한다 0과 1 사이에 값을 계속 작아지면 Underflow 문제가 발생하게 되는데
0으로 처리되어 정보손실이 발생할 수 있다 이를 해결하기 위해 자연 로그를 취하고 곱셈을 덧셈으로 바꾸면 Underflow 해결 가능
소수점이 있기 때문에 자연로그에 음수를 취하고(음수로 취하면 최대화 문제를 최소화 문제로 변환 가능) 평균으로 구하면 BCE를 얻을 수 있다
BCE는 두 가지 가능한 결과에 대한 Cross-Entropy라고 불린다. 웨이태에 미분해서 반대 방향으로 나아가면 loss를 줄이게 된다
로지스틱 회귀 (logistic regression)
입력과 출력 사이의 관계를 확률 함수로 표현하고 은닉층이 없는 인공신경망으로 놓고 추청하는 방법 입출력 사이의 관계를 logistic 함수 관계로 놓고
그 함수의 파라미터를 추정 logit(log-Odds Odds는 승리 확률이라고 불림)을 선형 회귀를 통해 구함
logit을 출력하는 신경망(선형 회귀)와 logit을 확률로 변환하는 Sigmoid함수로 BCE loss 계산할 수 있다 
로지스틱 회귀는 선형 회구를 통해 logit을 예측하고 이를 확률로 변환하여 이진 분류 문제를 해결하는 방법이
 
