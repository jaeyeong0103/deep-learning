그라덴트 함수에 문제점을 해결하기 위해 개발된 방법이 확률적 경사 하강법 (stochastic gradient descent)
GD는 모든 데이터를 고려하는 반면 SGD는 하나의 데이터를 무작위 하게 선택하여 손실은 계산함
주머니에서 무작위로 하나의 에러 제곱을 뽑고 loss로 사용한다
이 loss 기반으로 그래디언트를 계산하고 수식으로 파라미터를 업데이트 한다
이미 사용한 에러를 제외한 나머지 중 하나를 뽑는 것을 주머니가 빌 때까지 반복
주머니가 비면 다시 모든 데이터를 주머니를 넣고 반복
순환 한번당 episode 1 이라 한다 local minimum의 탈출의 기회가 됨
SGD가 GD보다는 신중하지는 않지만 빠르게 최저점을 도착할 수 있다
GD는 최저점을 향해 직선으로 나아가지만 SGD는 불규칙하게 나아간다
*그래디언트의 반대 방향이 항상 최저점으로 가는 방향이 아님!!!!*
그래디언트가 항상 + 방향으로 가지 않는 이유가 검은색 등고선이 모든 데이터를 고려한 전체 loss의 등고선이기 때문이다
