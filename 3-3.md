mini-batch SGD
SGD는 성급하게 방향을 결정하기고 편향된 데이터를 초래할 수 있기에 mini-batch SGD가 개발됨
복수의 데이터를 loss 계산을 포함시킴
두개의 데이터를 무작위로 뽑아 그 평균을 loss로 삼고 그래디언트를 계산하여 파라미터를 업데이트 하게 된다
그 후 다른 데이터 2개를 무작위로 뽑아 과정을 반복
GPU는 병렬 연산을 가능하게 함으로 여러 데이터를 써도 빠르다
batch size를 확장 시키면 전체적인 학습 속도를 상향 할 수 있지만 8k까지 제한을 해야한다 안좋은 local minimum에 수렴 가능성 존재, 에러 가능성 있음
learning rate warmup : 학습 초기에 0에서 시작하여 점진적으로 증가시킴
batch size를 키울 때 learning rate도 비례하여 키운다
linear scaling rule 에 따라 BS 두 배 하면 LR 두 배 해야 한다
룰을 적용하면서 그래디언트 크기가 커지면 learning rate warmup을 적용시킨다.
작은 bs 일 때 성능을 얻을 수 있다 큰 bs일수록 local minimum에 빠질 확률 존재
epoch : 전체 데이터를 몇번 반복하여 학습한 횟수
batch size : 한 번에 처리하는 데이터 개수
learning rate : 모델이 학습할 때 파라미터를 조정하는 값 (적정하게)
파라미터 : 모델이 학습을 통해 스스로 조정하는 값
하이퍼파라미터 : 학습 전 사람이 설정하는 초기 값 




