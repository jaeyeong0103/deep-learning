Universal Approximation Theorem 인공 신경망은 사실상 모든 함수를 표현할 수 있다
MLP의 능력은 히든 레이어가 있으면 제한된 범위 안의 어떤 연속 함수를 사용할 수 있다 MLP로 어떤 함수든 다 표현할 수 있다 즉 train loss를 0으로 만들어버릴 수 있다
데이터 포인트가 많다면 Unit step function을 활용하여각 데이터 포인트 마다 두개의 노드를 사용해서 수많은 네모 함수들을 쌓아 코싸인 그래프를 근사로 구할 수 있다
Unit step function을 비슷하게 만든 Sigmoid 경우 히든 레이어의 웨이트 값을 크게 하면 된다 다만 네모 함수 한개 만드는데 Unit step function는 2개 필요하지만 Sigmoid함수는 4개 필요하다
Unit step function은 딥러닝에서 편미분값을 0을 만들기에 학습이 불가능하게 한다는 오해가 있지만 이러한 특징으로 MLP를 걱정 없이 사용할 수 있다
실제로는 네모 함수를 만들기 위해 웨이트가 바이어스가 조정되는 것은 아님 네모 함수를 만들어 trainloss가 0이 되는 모델은 과적합을 야기한다
효율적인 인공지능을 만들기 위해 문제의 특성에 맞는 다양한 구조와 기법을 적절히 활용하는 것이다
