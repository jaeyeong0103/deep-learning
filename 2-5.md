local minimum과 계산이 많은 문제를 해결하기 위해 개발된 방법이 확률적 경사 하강법 SGD(stochastic gradient descent)이다
웨이트 초기화 (weight initailization) 
파라미터 초기값이 최소점으로부터 멀리 떨어져 있을수록 최적값에 도달하기 위해 더 많은 업데이트가 필요하기에
Yann LeCun Kaming He Xavier 방식으로 웨이트를 평균이 0인 렌덤한 값으로 초기화 시킨다.
바이어스도 0 또는 적은 양수로 초기화 시킨다
LeCun : 분포 평균과 분산은 동일하고 주변에 더 집중된 값을 선택한다
Kaming : 분포 평균 분산 동일하고 LeCun보다 2배 더 큰 분산을 가진다 ReLU를 사용함
Xavier : 분산이 Kaming 보다 작고 분포 평균 분산이 동일하다. sigmoid tanh 사용함
N in 값이 크면 그만큼 입력값이 웨이트와 곱해진 후 모두 더하지게 때문에 값과 분산이 커지고 분산이 커지면 학습이 느려지는 현상이 발생한다
입력값이 매우 크거나 작으면그래디언트가 0에 가까워 짐 (기울기 소실 문제)
분산이 작게끔 하는것이 목표 (적절하게 초기화 하자)
