Universal Approximation Theorem 인공 신경망은 사실상 모든 함수를 표현할 수 있다
MLP의 능력은 히든 레이어가 있으면 제한된 범위 안의 어떤 연속 함수를 사용할 수 있다 MLP로 어떤 함수든 다 표현할 수 있다 즉 train loss를 0으로 만들어버릴 수 있다
데이터 포인트가 많다면 Unit step function을 활용하여각 데이터 포인트 마다 두개의 노드를 사용해서 수많은 네모 함수들을 쌓아 코싸인 그래프를 근사로 구할 수 있다
Unit step function을 비슷하게 만든 Sigmoid 경우 히든 레이어의 웨이트 값을 크게 하면 된다 다만 네모 함수 한개 만드는데 Unit step function는 2개 필요하지만 Sigmoid함수는 4개 필요하다
Unit step function은 딥러닝에서 편미분값을 0을 만들기에 학습이 불가능하게 한다는 오해가 있지만 이러한 특징으로 MLP를 걱정 없이 사용할 수 있다
실제로는 네모 함수를 만들기 위해 웨이트가 바이어스가 조정되는 것은 아님 네모 함수를 만들어 trainloss가 0이 되는 모델은 과적합을 야기한다
효율적인 인공지능을 만들기 위해 문제의 특성에 맞는 다양한 구조와 기법을 적절히 활용하는 것이다

깊은 인공지능 신경망의 고질적 문제
기울기 소실 문제: 깊은 인공신경망에서 입력층에 가까운 층일수록 그래디언트 크기가 점점 작아지는 현상
업데이트가 진행될수록 점점 초기에 설정한 값에서 거의 변화가 없게 된다 따라서 첫번째 층 자체가 없데이트가 되어버리는데 입력층에 갈수록 그래디언트 크기가 0으로 가버리기에 학습이 이루어 지지 않는 기울기 소실이 생기게 된다 이는 깊은 인공지능일수록 피해가 더 커지게 된다(깊이가 작은 인공지능은 오히려 득이 될지도) 
해결 방법: ReLU 활성화 함수 적용: 들 값이 양수인 경우 입력값 그대로 출력하고 음수일 때는 0으로 출력하여 활성화 함수 미분값이 1 또는 0으로 한정되기에 그래디언트가 입력층에 갈수록 그래디언트가 줄어드는 현상을 방지할 수 있다 즉, 음수 입력에 대한 기울기를 학습 가능한 파라미터로 설정한다. (노드 개수가 충분히 많아야 해결 방법이 될 수 있다)이로 인해 일부 뉴런이 비활성화되고, 네트워크의 표현력이 감소함, 학습을 못하기에 적은 노드 수를 가진 인공지능에 사용하게 되면 학습이 불가능 해진다 
Sigmoid 함수와 비교해보면 sigmoid는 출력 값이 0과 1 사이에 제한되어 있어, 네트워크의 출력이 특정한 범위 내에 포함되지만 전파 과정에서 기울기가 소실되어 학습이 어려워지는 문제가 발생하게 된다. 그러므로 ReLU 계산보다 더 많은 비용을 처리해야한다
배치 정규화 기법 적용: 각 층 입력 분포가 학습 중에 변화해서 각 층은 새로운 분포에 적응하는 훈련을 한다 이는 각 층의 입력 분포가 안정화되는 효과를 가져올 수 있다
래이어 정규화 기법 적용: 모든 데이터에 대한 평균과 분산을 계산한다
Lose Landscape 문제: loss 함수가 형태가 복잡해져 최적의 해를 찾기 어려운 상태
Loss 함수 그래프에 모양이 점점 이상해져 간다 그래서 ResNet의 skip-connection을 추가하는 방법이 있다
과적합 문제: 모델이 훈련 데이터에 지나치게 적응해버려 새로운 데이터에 예측 능력이 떨어져 버린 상태
해결 방법: 데이터 증강 (데이터가 충분하지 않으면 답이 없다)기존 데이터를 최대한 활용하게 하여 인공지능에게 학습의 기회를 준다(90도 회전 반전 채도 조정 등등)
Dropout: 학습 과정에서 일부 노드를 무작위로 비활성화 시킨다 비활성화된 노드는 학습이 되지 않는다. 이는 모델이 특정 패턴에 의존하지 않도록 하면서 일반화 성능도 높일 수 있다 (테스트 과정에서는 모든 노드가 활성화됨) 그러나 일부 노드를 비활성화 하기에 훈련 시간이 늘어나고 하이퍼파라미터도 조정해야 한다
오토 인코더 실험: 비지도 학습 방식으로 데이터의 특성을 학습하는 신경망 구조
오토 인코더의 표면적인 목표는 입력된 값을 그대로 출력하고 입력한 정보를 효과적으로 압축해서 낮은차원에 고차원 정보를 담는 역할을 한다
인코더(Encoder): 노드의 개수가 점점 줄어들면서 정보를 압축한다
디코더(Decoder): 노드의 개수가 늘어나면서 정보를 복원한다
손실 함수(Loss Function): MSE Loss 함수를 적용함
Regularization: loss함수에 파라미터 크기를 추가하여 함께 고려하는 기법 모델의 복잡성을 줄이고 일반화 성능을 향상시켜준다. 
L1-Regularization: 손실 함수에 가중치의 절대값 합을 추가하는 방식, 가중치가 0이 되는 경우가 많아져 dropout 같은 특성을 보이게 된다. 이는 해석에 용이하고 불필요한 특성을 제거하는 효과를 가져올 수 있다(비선형)
L2-Regularization: 실 함수에 가중치의 제곱합을 추가하는 방식, 중치를 작게 만들어, 과적합을 방지하는 데 도움을 준다 l1과 다르게 가중치가 0이 되지는 않는다(선형)
