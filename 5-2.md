역전파(Backpropagation) 너무 어려움
깊은 웨이트의 대한 편미분을 구하기 위해 chain rule을 이용(중요) 인공지능 최적화 가능
chain rule이란 합성 함수의 미분을 구하는 원리로 각각 변수(patch)를 편미분한 결과를 곱해가는 과정이다
역전파는 파라미터에 대한 loss의 편미분을 효율적으로 계산하는 알고리즘 출력층 부터 시작하고 입력층 방향으로 진행함
loss를 모든 웨이트에 대한 편미분한 그라디언트로 반대방향으로 웨이트를 업데이트를 하며 딥러닝을 한다
편미분을 하면서 겹치는 항이 발생하게 되는데 이를 이용해서 중간 결과를 재사용하면 계산 효율이 올라간다
그러므로 더 깊은 층을 계산 할 수록 액티비션 미분, 웨이트, 액티비션 미분, 나1 등 액티베이션 웨이트가 추가되면서 계산이 된다
이런 계산은 기울기 소실 문제를 해결하게 된다. 깊은 층의 웨이트 일수록 많은 액티비션을 곱하지만 중간 결과를 활용하면 계산 효율성을 높이게 된다
역전파는 계산 효율성 증대 및 인공 신경망 작동 원리 파악 등 도움을 준다
순전파(forward propagation) 중요성
인공신경망의 학습은 초기 파라미터 값을 설정한 다음 loss가 충분히 작아질 때 까지 그래디언트를 이용 파라미터를 업데이트를 하는데
순전파는 각 노드의 들 나 값을 저장하는 역할을 한다 순전파를 활용해서 그래디언트를 계산하게 된다
순전파로 들 나 의 값을 저장하고 역전파에서 순저파 과정에서 저장한 값들을 활용해 그래디언트 계산
SGD Adam 알고리즘을 통해 파라미터를 업데이트 하면서 최적화를 한다

