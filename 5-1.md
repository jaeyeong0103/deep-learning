MLP 는 웨이트를 곱하고 바이어스와 함께 더하고 액티베이션의 연속
노드가 많을수록 곱하기 더하기 식이 많아지고 엑티비션까지 표현해야 함으로 수식 표현이 복잡해진다
행렬과 백터를 이용하면 간단하게 표현할 수 있는데 백터와 행렬의 곱으로 한번에 표현할 수 있다
MLP를 웨이트 행렬에 곱하고 바이어스 백터와 함께 더하고 액티베이션의 연속으로 간결하게 표현 가능
비선형 액티베이션의 중요성
MLP를 행렬과 백터로 표현하면서 더 많은 층과 노드를 사용하고 복잡한 함수를 표현하기 위해 활성화 함수로 비선형 함수를 사용해야 한다
선형 액티베이션을 사용하게 된다면 FC 레이어 층과 같은 복잡도를 가지게 되기 때문에 복잡해 질 수 없다 선형함수의 합성이 다시 선형 함수가 되기 때문이다
결과적으로 어느 값의 기울기 파라미터로 치환을 하게 되더라도 같은 성능을 가지게 되는 모델을 얻게 된다
비선형 액티베이션을 이용하면 입출력간의 비선형 관계를 이용할 수 있으며 깊어질 수록 복잡한 함수를 얻을 수 있다
비선형 액티베이션은 출력 값이 제한되거나 정보 손실이 발생할 수 있기에  마지막 층 혹은 중간에 선형 액티베이션을 사용하여 비선형 액티베이션이 가지는 리스크를 회피한다
MobileNetV2에서 도입된 아이디어는 노드 수가 줄어드는 레이어에서는 애티베이션을 하지 않음으로써 비선형성을 포기하면서 정보 손실을 막고
노드 수가 늘어나는 레이어에서 비선형 액티베이션을 사용함으로써 정보 손실을 최소화 하면서 비선형성을 확보하는 아이디어이다
현대 딥러닝에서는 선형 액티베이션과 비선형 액티베이션을 적절하게 섞어 나갈줄 알아야 한다



